---
title:
author: "cjlortie"
date: "May 2016"
output:
  html_document:
    theme: yeti
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
---
#BIOL5081: Biostatistics Intro
The first six weeks of the course will cover data science & fundamental exploratory data analysis in r.  It is a brief introduction to the contemporary open science, best-practice data and biostatistical working tools. The primary goal is exploration of tools and approaches associated with effective, efficient, reproducible biostatistical analyses. Inspirations include [software carpentry](http://software-carpentry.org), [rforcats](http://rforcats.net), and many, more open, online, free I can direct you to as needed. The [description](http://biology.gradstudies.yorku.ca/courses/biol-5081/) provided by the university is a useful starting point in deciding if the Fall 2016 offering meets your specific needs.

use.ful

bio.stats

adventure.time


[Course outline](https://github.com/cjlortie/r.stats/blob/gh-pages/BIOL5081-Fall2016.pdf)


![](./stats.JPG)

###Lesson 1. Data science (DS).
want data. have data. will collect data.
assumption: in this course, you are here to work with data.
data literacy IS data science.


[WhyR introductory lecture](http://www.slideshare.net/cjlortie/whyr)

[The importance of data viz](http://datascienceplus.com/the-importance-of-data-visualization/)

<b> Philosophy of R stats </b>
Statistical thinking: likelihood, error, and effect sizes. Contemporary statisticians embrace and are mindful of these three key concepts in all research, data, and statistical inference examinations.

Modes of inference with your data: using data, what can you infer or do?
1. Data description
2. Likelihood
3. Estimation
4. Baysian inference - weight and include what we know
5. Prediction
6. Hypothesis testing
7. Decision making -balance gains and risks

This set of ideas provide the foundation for many data science and statistical approached to working with evidence within almost every domain of science.

<b>Data viz first and foremost. Case study #1.</b>
```{r, data viz exercise}
#blog post by Fisseha Berhane
library(ggplot2)
library(dplyr)
library(reshape2)

#Create four groups: setA, setB, setC and setD.
setA=select(anscombe, x=x1,y=y1)
setB=select(anscombe, x=x2,y=y2)
setC=select(anscombe, x=x3,y=y3)
setD=select(anscombe, x=x4,y=y4)

#Add a third column which can help us to identify the four groups.
setA$group ='SetA'
setB$group ='SetB'
setC$group ='SetC'
setD$group ='SetD'

#merge the four datasets
all_data=rbind(setA,setB,setC,setD)  # merging all the four data sets
all_data[c(1,13,23,43),]  # showing sample

#compare summary stats
summary_stats =all_data%>%group_by(group)%>%summarize("mean x"=mean(x),
                                       "Sample variance x"=var(x),
                                       "mean y"=round(mean(y),2),
                                       "Sample variance y"=round(var(y),1),
                                       'Correlation between x and y '=round(cor(x,y),2)
                                      )
models = all_data %>% 
      group_by(group) %>%
      do(mod = lm(y ~ x, data = .)) %>%
      do(data.frame(var = names(coef(.$mod)),
                    coef = round(coef(.$mod),2),
                    group = .$group)) %>%
dcast(., group~var, value.var = "coef")

summary_stats_and_linear_fit = cbind(summary_stats, data_frame("Linear regression" =
                                    paste0("y = ",models$"(Intercept)"," + ",models$x,"x")))

summary_stats_and_linear_fit

#data viz instead as first step
ggplot(all_data, aes(x=x,y=y)) +geom_point(shape = 21, colour = "red", fill = "orange", size = 3)+
    ggtitle("Anscombe's data sets")+geom_smooth(method = "lm",se = FALSE,color='blue') + 
    facet_wrap(~group, scales="free")

```
Outcome from stats first, data viz later (tricked), descriptive estimates of data can be deceptive. Draw first, then interpret.


<b>Survey data from class. Case study #2.</b>
```{r, survey}
#load class survey responses from google poll completed in class
survey<-read.csv("data/5081.survey.1.csv")
str(survey) #check data match what we collected

#data viz
hist(survey$r.experience, xlab="experience in R (1 is none, 5 is extensive)", ylab="frequency", main="Likert Scale 1 to 5")
plot(survey$r.experience~survey$discipline, xlab="discipline", ylab="experience in R")
plot(survey$r.studio, ylab="R Studio")
plot(survey$research.data, ylab="Research data")
#observe patterns by checking plots
```
<b>Observations from data viz</b>
We have limited experience in R. Experience in R varies by research discipline. A total of half the respondents have tried R Studio. Most participants will be working with quantitative data in their own research projects.

```{r, test survey data with EDA}
#Now, try some simple summary statistics.
summary(survey)
#Data summary looks reasonable based on plots, mean R experience is < 2
t.test(survey$r.experience, mu=1) #t-test if mean is different from 1
t.test(survey$r.experience, mu=2) #t-test if mean is different from 2
#A one sample t-test confirms we have a bit experience in R.

m1<-glm(r.experience~discipline, family = poisson, data = survey) #test for differenes between disciplines in R experience
m1 #model summary
anova(m1, test="Chisq") #test whether the differences in model are different
#Too little evidence to be significantly different between disciplines.

```

<b> Practical skill outcomes of R stats useful for competency test</b>
Understand the difference between R and R Studio.
Use scripts or R Markdown files to save all your work.
Be prepared to share you code.
Load data, clean data, visualize data, then and only then begin applying statistics.
Proximately: be able to use and work with dataframes, vectors, functions, and libraries in R.

###Lesson 2. Workflows & Data Wrangling (WDW).
worflows
reproduce. 
openly.

data wrangling
more than half the battle.

![](./beemo.rodeo.png)

[Data wrangling slide deck](http://www.slideshare.net/cjlortie/data-wrangling-in-r-be-a-wrangler)

<b> Philosophy of R stats </b>
Tidy data make your life easier. Data strutures should match intuition and common sense. Data should have logical structure.  Rows are are observations, columns are variables. Tidy data also increase the viability that others can use your data, do better science, reuse science, and help you and your ideas survive and thrive. A workflow should also include the wrangling you did to get your data ready. If data are already very clean in a spreadsheet, they can easily become a literate, logical dataframe. Nonetheless, you should still use annotation within the introductory code to explain the meta-data of your data to some extent and what you did pre-R to get it tidy.  The philosophy here is very similar to the data viz lesson forthcoming with two dominant paradigms. Base R code functions, and pipes %>% and the logic embodied within the libraries associated with the the tidyverse. Generally, I prefer the tidyverse because it is more logical and much easier to remember.  It also has some specific functions needed to address common errors in modern data structures with little code needed to fix them up.

<b>Worflow</b>
template for r-script
meta-data ####
author: cjlortie
date: 2016
purpose: 

set-up ####
rm(list=ls())
getwd()
setwd("~/r")

read & check data ####
data <-read.csv(“filename.csv”)
names()
dim()
str()
summary(data)

visualize ####

check assumptions ####

model data and test hypothesis ####

<b>Data wrangling</b>

<b>Base R</b>
key concepts:
aggregate
tapply
sapply 
lappy
subsetting
as.factor
is.numeric
na

<b>tidyverse</b>
key concepts:
pipes are you best friend!
%>% 

dplyr
filter for rows
select for columns
mutate for new variables
summarise for bringing together many values

[Excellent list of wrangling tools](http://www.computerworld.com/article/2921176/business-intelligence/great-r-packages-for-data-import-wrangling-visualization.html)


[Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)

[tidyr](https://cran.r-project.org/web/packages/tidyr/README.html)

[Great wrangling webinar](https://www.rstudio.com/resources/webinars/data-wrangling-with-r-and-rstudio/)

We will cover two basic challenges you will certainly encounter.
<b>Missing data </b>
```{r, missing data}
#Missing data. In error, missing cells/observations in some measure can kick back an error. In other apps, sometimes ignored but can introduce error.
ttc <-read.csv("data/ttc.csv")
str(ttc)

#check for missing values
is.na(ttc)
summary(ttc, na.rm=TRUE) #excludes NA
new.ttc <-na.omit(ttc) # returns without missing values
is.na(new.ttc) #check to see if it worked

#many other solutions but I use these two frequently

```

<b>Selecting part of a dataset </b>
```{r, selections}
library(dplyr)
survey<-read.csv("data/5081.survey.1.csv")
str(survey)

#I want just a simple dataframe with experience by discipline

experience <- survey %>% select(discipline, r.experience)
experience

#Now I just want to select the physiology folks
physiologists <- experience %>% filter(discipline == "physiology")
physiologists

#Selections also often include a summary by levels or I want to make a new column with some calculations. Think about what you have likely done in excel.

#used pipes and made a nice summary table
experience <-survey %>% group_by(discipline) %>% summarise(
  count = n(),
  exp = mean (r.experience)
)

#What if I just want to make a new column to my dataframe that is a sum, a calculation, or some addition
ttc.5.years <- ttc %>% mutate(five.year.sum = X2015+X2014+X2013+X2012+X2011)
str(ttc.5.years)
str(ttc)

#so we made a new column.
#can do this to original dataframe too without making new object
ttc %>% mutate(five.year.sum = X2015+X2014+X2013+X2012+X2011)
ttc
#notice any errors? :)

```

<b> Practical skill outcomes of R stats useful for competency test</b>
Check and address missing values.
Grab part of a dataset.
Use pipes to move/wrangle chunks of your dataset

###Lesson 3. Visualization in r (VR).
basic plots.
lattice.
ggplot2.
you need to see data. see the trends. explore them using visuals.

[Contemporary data viz for statistical analyses slide deck](http://www.slideshare.net/cjlortie/data-visualization-in-r-65991145)

<b> Philosophy of R stats </b>
Clean simple graphics are powerful tools in statistics (and in scientific communication).  Tufte and others have shaped data scientists and statisticians in developing more libraries, new standards, and assumptions associated with graphical representations of data.  Data viz must highlight the differences, show underlying data structures, and provide insights into the specific research project. R is infinitely customizable in all these respects.  There are at least two major current paradigms (there are more these are the two dominant idea sets).  Base R plots are simple, relatively flexible, and very easy. However, their grammar, i.e their rules of coding are not modern. Ggplot and related libraries invoke a new, formal grammar of graphics that is more logical, more flexible, but divergent from base R code. It is worth the time to understand the differences and know when to use each.

Evolution of plotting in statistics using R in particular went from base-R then onto lattice then to the ggvis universe with the most recent library being ggplot2. Base-R is certainly useful in some contexts as is the lattice and lattice extra library. However, ggplot2 now encompasses all these capacities with a much simpler set of grammar (i.e. rules and order). Nonetheless, you should be able to read base-R code for plots and be able to do some as well. The philosophy or grammar of modern graphics is well articulated and includes the following key principles.

The grammar of graphics
layers
primacy of layers (simple first, then more complex) i.e. you build up your plots
data are mapped to aesthetic attributes and geometric objects
data first then statistics even in plots

Disclaimer: I love the power of qplots.

[ggplot2 cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf)

<b>Data viz case study #1.</b>
```{r, survey data viz}
library(ggplot2)
survey<-read.csv("data/5081.survey.1.csv")
str(survey)

plot(survey$r.experience) #hard to tell what is going on

qplot(r.experience, data=survey) #decided to make bins for me and count up)

#so, we know better and instead do a histogram using base graphics
#basic data viz for EDA
hist(survey$r.experience) #better
qplot(r.experience, data=survey, geom="histogram") #same as what is picked for us
qplot(r.experience, data=survey, geom="histogram", binwidth=0.5)

barplot(survey$r.experience) #confusing
qplot(r.experience, data=survey, geom="bar") #what, it is back!

#basic data viz for EDA but for interactions
plot(survey$discipline, survey$r.experience)
qplot(discipline, r.experience, data=survey) #not the same
qplot(discipline, r.experience, data=survey, geom="boxplot")

plot(survey$r.studio~survey$r.experience) #ugly
qplot(r.experience, r.studio, data=survey) #useless
qplot(r.studio, data=survey, weight = r.experience) #sweet new feature here

#ok, so you get it. grammar different, visuals about the same for super quick, simple plots. The grammar hints at the power that awaits though.

#grammar different, simple x or x~y plots about the same

```

<b>Data viz case study #2.</b>
```{r, diamonds are our best friend}
str(diamonds)
#crazy number of observations. We need less. too many riches not always good.
set.seed(1000)
dsmall<-diamonds[sample(nrow(diamonds), 100), ]

plot(dsmall$carat, dsmall$price)
qplot(carat, price, data=dsmall)

#ok no difference
#now let's see what we can do with qplot with a few bits added
#one little argument extra added
qplot(carat, price, data = dsmall, colour = color)
qplot(carat, price, data = dsmall, shape = cut)

#how about using data viz to even more thoroughly explore potential stats we could do.
#qplots - quick plot, thoughful build of layers
qplot(carat, price, data = dsmall, geom = c("point", "smooth"))

#what about trying some stats on this now, at least from a viz philosophy
qplot(color, price / carat, data = dsmall, geom = "boxplot") #can include formulas and methods

#or check for proportional differences
qplot(carat, data = dsmall, geom = "histogram", fill = color) #to see proportions
qplot(carat, data = dsmall, geom = "histogram", weight = price) # weight by a covariate
     
#final idea, how about subsetting with the data right in the code for the plots!
qplot(carat, data = diamonds, facets = color ~ .,
  geom = "histogram", binwidth = 0.1, xlim = c(0, 3)) #to compare between groups

#qplot is so powerful.
#colour, shape and size have meaning in the R code from this library
#layers added for you by qplots

#qplot gets you 2x and one y or one x and 2y so >2 variables at once easily
```

<b>Data viz case study #3.</b>
```{r, ggplot}
#GGPLOT() gives you even more options for adding layers/options
p <- ggplot(mtcars, aes(x = mpg, y = wt))
p + geom_point()

#now play time with this case study.
#try out some geom options and different aesthetics and make some errors.
#prize for the prettiest plots

#displ is car engine size in Liters
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = class))

#so aethetics are one way to add variables in and expand your plotting power
#however facets are another way to make multiple plots BY a factor

#facet wrap is by one variable
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 2)

#facet_wrap(~cell) - univariate: create a 1-d strip of panels, based on one factor, and wrap the strip into a 2-d matrix
#facet_grid(row~col) - (usually) bivariate: create a 2-d matrix of panels, based on two factors

#facet grid is by two variables
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ cyl)

#another example more perfect code
p <- ggplot(data = mpg, aes(x = displ, y = hwy, color = drv)) + geom_point()
p + facet_wrap(~cyl)

#or just use facets in qplot but much simpler
qplot(displ, hwy, data=mpg, facets = . ~ year) + geom_smooth()

```

<b>Data viz case study #4.</b>
```{r, worked example}
#try it with ecological.footprints.csv
footprints <-read.csv("data/ecological.footprints.csv")
str(footprints)
#aha, R thinks year is an integet
footprints$yr <- as.factor(footprints$yr)
library(ggplot2)
qplot(country, ecological.footprint, data = footprints) #too messy

qplot(country, ecological.footprint, data = footprints, colour = yr) #better but still a lot to process

qplot(country, ecological.footprint, data = footprints, facets = yr~.) #ok but do not love. hard to see distribution

qplot(ecological.footprint, data = footprints) #you know what, this is not bad.  maybe add year in too/

qplot(ecological.footprint, data = footprints, fill = yr) #ok now I starting to see structure and differences

#OK, now I am ready for stats. Thinking about these data, I see we have only two years for some countries so cannot do within country or between country contrasts. So, most likely hypothesis I can test is whether ecological footprints are increasing between these two years. Not a perfect dataset really but could compare these two years.

t.test(footprints$ecological.footprint~ footprints$yr)
#ok looks like there are differences between years but it is hard to tell from previous plot. Realize now, I need a better plot still?

qplot(yr, ecological.footprint, data = footprints, geom="boxplot") #this is weird, 2000 looks higher

#different countries between years?
#more countries reported in 2000?
library(dplyr)
footprints %>% count(yr)
#Yup, way more data for year 2000
#maybe should just the countries that were testedin both years?

library(tidyr)
matches <- spread(footprints, yr, ecological.footprint) %>% filter() %>% na.omit
str(matches)
matches

#whoa, USA and Canada missing, and we have HUGE footprints.

#got it. just the countries with measure in BOTH years.
#now, gather up again with these filtered matches!

new <- matches %>% gather(`2000`, `2012`, key = "yr", value ="ecological.footprint")
new #ok so now a nice dataframe with just matches back in a format I can use for plots and stats

qplot(yr, ecological.footprint, data = footprints, geom="boxplot")

t.test(new$ecological.footprint~ new$yr, paired=TRUE)
#Well, I give up, seems like the footprints for the world went down not up in this time period. GOOD NEWS for the environmental movement in some respects.
new %>% group_by(yr) %>% summarise(mean(ecological.footprint))

# We still use 2.4 planets worth of resources but only have one.

#AND, we are missing some key countries that contribute to global change including Canada and USA.

```

<b> Practical skill outcomes of R stats useful for competency test</b>
Do meaningful plot of a single variable wth nice labels.
Do a meaningful plot of a relationship between two variables.
Use qplots to do a plot that includes an aesthetic.
Do a ggplot that includes three variables.

(Physics example in Wired Mag why to plot data even if you have model)[https://www.wired.com/2016/09/might-gotten-little-carried-away-physics-time/]
 
###Lesson 4. Exploratory data analysis (EDA) in r.
fundamental descriptive stats.
distributions via density plots
GLM. GLMM. Post hoc tests.
modelR

[slide deck for EDA]()

<b> Philosophy of R stats </b>


[EDA from a data science perspective](http://r4ds.had.co.nz/exploratory-data-analysis.html#introduction-3)



###Lesson 5. Wrangle, visualize, and analyze.
A graduate-level dataset.
Apply your new mathemagical skills from scratch.
A single three-hour block.
As advanced as you want to be.
Fundamental principles demonstrated.
At least two fundamental statistical tests demonstrated.

<b>Halloween Hackathon</b>
Costumes options.
Candy for sure.
Pizza Provided.
Now we practice from start to finish including submission of the r-script or rmd you write to turnitin.com

<b>Practical outcomes to demonstrate </b>
Rename some variables.
Address missing values.
Generate a simplified table/dataframe. 
Visualize the data to identify patterns and relationships.
Produce a publication quality graphic.
Do EDA on data very broadly.
Do a single statistical address to capture main effect observed.
Annotate

<b>Rubric</b>
A total of 25% so 5 questions each worth 5 points.
Likert Scale of 1 to 5 with 1 being low and 5 being awesome.

(1) Can I understand what was done?
(2) Can I repeat what was done?
(3) Does the data viz effectively and thoroughly examine and show patterns/relations?
(4) Is the EDA a clear and appropriate examination the evidence and demonstrates statistical thinking?
(5) Is the final graphic and statistical test appropriate (tidy, polished, and meaningful) and suitable for publication?

###Lesson 6. Competency test.
deliberate practice.
tested with feedback.
a virtual repeat of last week but evaluated



